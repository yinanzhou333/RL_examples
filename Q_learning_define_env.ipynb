{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ebda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb57187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnvironment:\n",
    "    def __init__(self):\n",
    "        self.num_states = 9\n",
    "        self.num_actions = 4\n",
    "        self.current_state = 0  # Initial state\n",
    "        self.goal_state = 8\n",
    "        self.pit_state = 5\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode is already done. Call reset() to start a new episode.\")\n",
    "\n",
    "        # Define the grid world transitions\n",
    "        transitions = {\n",
    "            0: {'up': 0, 'down': 3, 'left': 0, 'right': 1},\n",
    "            1: {'up': 1, 'down': 4, 'left': 0, 'right': 2},\n",
    "            2: {'up': 2, 'down': 5, 'left': 1, 'right': 2},\n",
    "            3: {'up': 0, 'down': 6, 'left': 3, 'right': 4},\n",
    "            4: {'up': 1, 'down': 7, 'left': 3, 'right': 5},\n",
    "            5: {'up': 2, 'down': 8, 'left': 4, 'right': 5},\n",
    "            6: {'up': 3, 'down': 6, 'left': 6, 'right': 7},\n",
    "            7: {'up': 4, 'down': 7, 'left': 6, 'right': 8},\n",
    "            8: {'up': 5, 'down': 8, 'left': 7, 'right': 8}\n",
    "        }\n",
    "\n",
    "        # Perform the action and get the next state\n",
    "        next_state = transitions[self.current_state][action]\n",
    "\n",
    "        # Update the current state\n",
    "        self.current_state = next_state\n",
    "\n",
    "        # Check if the agent reached the goal or fell into the pit\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 10  # Positive reward for reaching the goal\n",
    "            self.done = True\n",
    "        elif next_state == self.pit_state:\n",
    "            reward = -5  # Negative reward for falling into the pit\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return next_state, reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        self.done = False\n",
    "        return self.current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7194ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.561  7.29   6.561  7.29 ]\n",
      " [ 7.29   8.1    6.561  6.561]\n",
      " [ 6.561 -5.     7.29   6.561]\n",
      " [ 6.561  8.1    7.29   8.1  ]\n",
      " [ 7.29   9.     7.29  -5.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 7.29   8.1    8.1    9.   ]\n",
      " [ 8.1    9.     8.1   10.   ]\n",
      " [ 0.     0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = GridWorldEnvironment()\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "q_table = np.zeros((env.num_states, env.num_actions))\n",
    "# Define action index mapping\n",
    "action_index = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.9\n",
    "num_episodes = 1000\n",
    "\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment for a new episode\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action based on epsilon-greedy policy\n",
    "        epsilon = 1 # explore only\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "        else:\n",
    "            state = int(state)\n",
    "            #action = ['up', 'down', 'left', 'right'][np.argmax(q_table[state, :])]\n",
    "            # Choose randomly among actions with the maximum Q-value\n",
    "            max_actions = np.random.choice(np.where(q_table[state, :] == np.max(q_table[state, :]))[0])\n",
    "            action = ['up', 'down', 'left', 'right'][max_actions]\n",
    "\n",
    "\n",
    "        # Take the chosen action and observe the new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-value using the Q-learning update rule\n",
    "        q_table[state, action_index[action]] += learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action_index[action]])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d104700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Path: [0, 3, 4, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the learned Q-table to navigate from the start state to the goal state\n",
    "state = env.reset()  # Reset the environment\n",
    "path = [state]\n",
    "max_steps = 100\n",
    "while not env.done and len(path) < max_steps:  # Goal state\n",
    "    state = int(state)\n",
    "    action = ['up', 'down', 'left', 'right'][np.random.choice(np.where(q_table[state, :] == np.max(q_table[state, :]))[0])]\n",
    "    next_state, _, _, _ = env.step(action)\n",
    "    path.append(next_state)\n",
    "    state = next_state\n",
    "\n",
    "print(\"Optimal Path:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "909a8530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Paths Explored: 3\n",
      "Unique Path 1: (0, 1, 4, 7, 8)\n",
      "Unique Path 2: (0, 3, 6, 7, 8)\n",
      "Unique Path 3: (0, 3, 4, 7, 8)\n"
     ]
    }
   ],
   "source": [
    "# Run the environment multiple times and store unique paths\n",
    "num_runs = 100\n",
    "unique_paths = set()\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    state = env.reset()  # Reset the environment\n",
    "    path = [state]\n",
    "    max_steps = 100\n",
    "\n",
    "    while not env.done and len(path) < max_steps:  # Goal state\n",
    "        state = int(state)\n",
    "        action = ['up', 'down', 'left', 'right'][np.random.choice(np.where(q_table[state, :] == np.max(q_table[state, :]))[0])]\n",
    "        next_state, _, _, _ = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "\n",
    "    # Convert the path to a tuple and add it to the set to ensure uniqueness\n",
    "    unique_paths.add(tuple(path))\n",
    "\n",
    "# Print the number of unique paths explored\n",
    "print(\"Number of Unique Paths Explored:\", len(unique_paths))\n",
    "\n",
    "# Optionally, print or analyze the unique paths themselves\n",
    "for idx, path in enumerate(unique_paths):\n",
    "    print(f\"Unique Path {idx + 1}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b21d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
